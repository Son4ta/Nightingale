# query_rag.py (Optimized with Step-Back Query Rewriting)

import os
from dotenv import load_dotenv

from langchain.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from openai import OpenAI

# --- 1. é…ç½®åŒºåŸŸ ---
load_dotenv()

SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
if not SILICONFLOW_API_KEY:
    raise ValueError("SILICONFLOW_API_KEY not found in environment variables.")

SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"

EMBEDDING_MODEL = "Qwen/Qwen3-Embedding-8B"
LLM_MODEL = "Qwen/Qwen3-30B-A3B"
REWRITE_MODEL = "Qwen/Qwen3-30B-A3B" 

FAISS_INDEX_PATH = "knowledge_base"
RETRIEVAL_TOP_K = 5 # ä¸ºæ¯ä¸ªæ£€ç´¢å™¨è·å– Top K ä¸ªç»“æœ

# --- 2. RAG æ ¸å¿ƒé€»è¾‘ (å·²æŒ‰è¦æ±‚ä¼˜åŒ–) ---

# !!! ä¼˜åŒ–ç‚¹: æ–°å¢ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨äºè°ƒç”¨LLMè¿›è¡ŒæŸ¥è¯¢æ”¹å†™ !!!
def rewrite_query_with_llm(question: str) -> str:
    """
    ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œâ€œé€€å›å¼â€æŸ¥è¯¢æ”¹å†™ã€‚
    å°†å…·ä½“é—®é¢˜æŠ½è±¡æˆä¸€ä¸ªæ›´é€šç”¨ã€æ›´é€‚åˆè¯­ä¹‰æ£€ç´¢çš„é—®é¢˜ã€‚
    """
    client = OpenAI(api_key=SILICONFLOW_API_KEY, base_url=SILICONFLOW_BASE_URL)
    
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„å…·ä½“é—®é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªæ›´é€šç”¨ã€æ›´å®è§‚çš„â€œé€€å›å¼â€(Step-back)é—®é¢˜ã€‚è¿™ä¸ªé€€å›å¼é—®é¢˜åº”è¯¥æ•æ‰åŸå§‹é—®é¢˜çš„æ ¸å¿ƒæ¦‚å¿µå’Œæ„å›¾ï¼Œä½†æ›´é€‚åˆç”¨äºåœ¨å¤§è§„æ¨¡çŸ¥è¯†åº“ä¸­è¿›è¡Œé«˜å±‚æ¬¡çš„è¯­ä¹‰æ£€ç´¢ã€‚è¯·åªè¿”å›è¿™ä¸ªé€€å›å¼é—®é¢˜ï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ–å¤šä½™çš„æ–‡å­—ï¼Œä¿æŒè‹±æ–‡ç¼©å†™ä¸éœ€è¦ç¿»è¯‘"
    
    user_prompt = f"åŸå§‹é—®é¢˜: {question}\n\né€€å›å¼é—®é¢˜:"

    try:
        response = client.chat.completions.create(
            model=REWRITE_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.0,
            max_tokens=100,
            timeout=60,
        )
        rewritten_query = response.choices[0].message.content.strip()
        return rewritten_query if rewritten_query else question
    except Exception as e:
        print(f"\n[Warning] Query rewrite failed: {e}. Falling back to original query.")
        return question

def format_docs(docs):
    """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
    return "\n---\n".join(
        f"æ¥æº: {doc.metadata['source']}, é¡µç : {doc.metadata['page']}\nå†…å®¹: {doc.page_content}"
        for doc in docs
    )

print("Initializing models and retrievers...")

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embeddings = OpenAIEmbeddings(
    model=EMBEDDING_MODEL,
    openai_api_key=SILICONFLOW_API_KEY,
    openai_api_base=SILICONFLOW_BASE_URL
)

# åŠ è½½FAISSå‘é‡æ£€ç´¢å™¨
try:
    db = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)
    faiss_retriever = db.as_retriever(search_kwargs={"k": RETRIEVAL_TOP_K})
except Exception as e:
    print(f"Fatal: Could not load FAISS index from '{FAISS_INDEX_PATH}'. Error: {e}")
    exit()

# åˆå§‹åŒ–å…³é”®è¯æ£€ç´¢å™¨ (BM25)
doc_list = list(db.docstore._dict.values())
print("Initializing BM25 keyword retriever...")
bm25_retriever = BM25Retriever.from_documents(doc_list)
bm25_retriever.k = RETRIEVAL_TOP_K

# Prompt æ¨¡æ¿ (ä¿æŒä¸å˜)
template = """
ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è¯¾ç¨‹åŠ©æ•™ï¼Œä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æˆ‘æä¾›çš„ä¸Šä¸‹æ–‡ææ–™æ¥å›ç­”é—®é¢˜ã€‚
**æŒ‡ä»¤**:
1.  **ä¸¥æ ¼ä¾æ®ææ–™**: ä½ çš„å›ç­”å¿…é¡»å®Œå…¨åŸºäºä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ææ–™ï¼Œç¦æ­¢ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è¯†ã€‚
2.  **ç²¾ç¡®å¼•ç”¨**: å¯¹ä½ å›ç­”ä¸­çš„æ¯ä¸€ä¸ªè§‚ç‚¹æˆ–æ•°æ®ï¼Œéƒ½å¿…é¡»åœ¨å¥æœ«ç”¨ `(æ¥æº: <æ–‡ä»¶å>, é¡µç : <é¡µç >)` çš„æ ¼å¼æ³¨æ˜å‡ºå¤„ã€‚å¦‚æœä¸€ä¸ªè§‚ç‚¹ç»¼åˆäº†å¤šä¸ªæ¥æºï¼Œè¯·å…¨éƒ¨åˆ—å‡ºã€‚
3.  **å…¨é¢è€Œç®€æ´**: ç»¼åˆæ‰€æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç»™å‡ºå…¨é¢è€Œç²¾ç‚¼çš„å›ç­”ã€‚
4.  **æœªçŸ¥åˆ™ç­”æ— **: å¦‚æœä¸Šä¸‹æ–‡ææ–™ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œå¿…é¡»æ˜ç¡®å›ç­”ï¼šâ€œæ ¹æ®æä¾›çš„ææ–™ï¼Œæ— æ³•å›ç­”æ­¤é—®é¢˜ã€‚â€
**ä¸Šä¸‹æ–‡ææ–™**:
---
{context}
---
**é—®é¢˜**: {question}
**ä½ çš„å›ç­”**:
"""
prompt = PromptTemplate.from_template(template)

# åˆå§‹åŒ– LLM (ä¿æŒä¸å˜)
llm = ChatOpenAI(
    model_name=LLM_MODEL,
    openai_api_key=SILICONFLOW_API_KEY,
    openai_api_base=SILICONFLOW_BASE_URL,
    temperature=0.1,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]
)

# å®šä¹‰æœ€ç»ˆçš„ RAG é“¾
rag_chain = (
    prompt
    | llm
    | StrOutputParser()
)

# --- 3. äº¤äº’å¼æŸ¥è¯¢ (å·²ä¼˜åŒ–) ---
if __name__ == "__main__":
    print("\n--- RAG Exam Assistant Ready (Optimized) ---")
    print(f"   LLM Model: {LLM_MODEL}")
    print(f"   Embedding Model: {EMBEDDING_MODEL}")
    print("---------------------------------------------")
    print("Enter your question, or type 'exit' to quit.")
    
    while True:
        original_question = input("\n[è€ƒè¯•é—®é¢˜] > ")
        if original_question.lower() == 'exit':
            break
        if not original_question:
            continue
            
        # !!! ä¼˜åŒ–ç‚¹: æ­¥éª¤ 1 - è¿›è¡ŒæŸ¥è¯¢æ”¹å†™ !!!
        print("\nğŸ”„ [1. æ­£åœ¨è¿›è¡Œé€€å›å¼æŸ¥è¯¢æ”¹å†™...]")
        rewritten_question = rewrite_query_with_llm(original_question)
        print(f"   â””â”€ è¯­ä¹‰æ£€ç´¢æŸ¥è¯¢: \"{rewritten_question}\"")

        # !!! ä¼˜åŒ–ç‚¹: æ­¥éª¤ 2 - åˆ†åˆ«è¿›è¡Œè¯­ä¹‰å’Œå…³é”®è¯æ£€ç´¢ !!!
        print("\nğŸ” [2. æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...]")
        # ä½¿ç”¨â€œé€€å›é—®é¢˜â€è¿›è¡Œè¯­ä¹‰æœç´¢
        semantic_docs = faiss_retriever.invoke(rewritten_question)
        print(f"   â”œâ”€ è¯­ä¹‰æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(semantic_docs)} ä¸ªç»“æœã€‚")
        # ä½¿ç”¨â€œåŸå§‹é—®é¢˜â€è¿›è¡Œå…³é”®è¯æœç´¢
        keyword_docs = bm25_retriever.invoke(original_question)
        print(f"   â””â”€ å…³é”®è¯æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(keyword_docs)} ä¸ªç»“æœã€‚")

        # !!! ä¼˜åŒ–ç‚¹: æ­¥éª¤ 3 - åˆå¹¶å¹¶å»é‡æ£€ç´¢ç»“æœ !!!
        all_docs = semantic_docs + keyword_docs
        unique_docs_dict = {}
        for doc in all_docs:
            # ä½¿ç”¨é¡µé¢å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦
            unique_docs_dict[doc.page_content] = doc
        
        retrieved_docs = list(unique_docs_dict.values())
        print(f"\nğŸ“š [3. åˆå¹¶å»é‡åï¼Œå…±æ‰¾åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³ä¸Šä¸‹æ–‡]")
        if retrieved_docs:
            for i, doc in enumerate(retrieved_docs):
                source = doc.metadata.get('source', 'Unknown')
                page = doc.metadata.get('page', 'N/A')
                print(f"  [ç‰‡æ®µ {i+1}] æ¥æº: {source}, é¡µç : {page}")
                print(f"  å†…å®¹é¢„è§ˆ: {doc.page_content[:50].replace(chr(10), ' ')}...")
        
        # æ­¥éª¤ 4: åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
        print("\nğŸ§  [4. æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...]\n")
        
        context_str = format_docs(retrieved_docs)
        
        # è°ƒç”¨ RAG é“¾ï¼Œæµå¼å›è°ƒä¼šè‡ªåŠ¨æ‰“å°ç­”æ¡ˆ
        rag_chain.invoke({
            "context": context_str,
            "question": original_question # !!! å…³é”®ï¼šç”¨åŸå§‹é—®é¢˜å‘LLMæé—® !!!
        })
        print() # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ